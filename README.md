## MRCï¼ˆæœºå™¨é˜…è¯»ç†è§£ï¼‰å¼€æºä»£ç è§£æ

## Contents

[TOC]


## ä¸€ã€åŸºäºå¤§è§„æ¨¡MRCæ•°æ®å†è®­ç»ƒ

æ­¤åº“å‘å¸ƒçš„å†è®­ç»ƒæ¨¡å‹ï¼Œåœ¨ é˜…è¯»ç†è§£/åˆ†ç±» ç­‰ä»»åŠ¡ä¸Šå‡æœ‰å¤§å¹…æé«˜<br/>
ï¼ˆå·²æœ‰å¤šä½å°ä¼™ä¼´åœ¨ Dureaderã€æ³•ç ”æ¯ã€åŒ»ç–—é—®ç­” ç­‰å¤šä¸ªæ¯”èµ›ä¸­å–å¾—**top5**çš„å¥½æˆç»©ğŸ˜ï¼‰

|                æ¨¡å‹/æ•°æ®é›†                 |  Dureader-2021  |  tencentmedical |
| ------------------------------------------|--------------- | --------------- |
|                                           |    F1-score    |    Accuracy     |
|                                           |  dev / Aæ¦œ     |     test-1      |
| macbert-large (å“ˆå·¥å¤§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹)         | 65.49 / 64.27  |     82.5        |
| roberta-wwm-ext-large (å“ˆå·¥å¤§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹) | 65.49 / 64.27  |     82.5        |
| macbert-large (ours)                      | 70.45 / **68.13**|   **83.4**    |
| roberta-wwm-ext-large (ours)              | 68.91 / 66.91   |    83.1        |


* **æ•°æ®æ¥æº**
  * ç½‘ä¸Šæ”¶é›†çš„å¤§é‡ä¸­æ–‡MRCæ•°æ®
  ï¼ˆå…¶ä¸­åŒ…æ‹¬å…¬å¼€çš„MRCæ•°æ®é›†ä»¥åŠè‡ªå·±çˆ¬å–çš„ç½‘é¡µæ•°æ®ç­‰ï¼Œ
  å›Šæ‹¬äº†åŒ»ç–—ã€æ•™è‚²ã€å¨±ä¹ã€ç™¾ç§‘ã€å†›äº‹ã€æ³•å¾‹ã€ç­‰é¢†åŸŸã€‚ï¼‰

* **æ•°æ®æ„é€ **
  * æ¸…æ´—
    * èˆå¼ƒï¼šcontext>1024çš„èˆå¼ƒã€question>64çš„èˆå¼ƒã€ç½‘é¡µæ ‡ç­¾å æ¯”è¶…è¿‡30%çš„èˆå¼ƒã€‚
    * é‡æ–°æ ‡æ³¨ï¼šè‹¥answer>64ä¸”ä¸å®Œå…¨å‡ºç°åœ¨æ–‡æ¡£ä¸­ï¼Œåˆ™é‡‡ç”¨æ¨¡ç³ŠåŒ¹é…: è®¡ç®—æ‰€æœ‰ç‰‡æ®µä¸answerçš„ç›¸ä¼¼åº¦(F1å€¼)ï¼Œå–ç›¸ä¼¼åº¦æœ€é«˜çš„ä¸”é«˜äºé˜ˆå€¼ï¼ˆ0.8ï¼‰
  * æ•°æ®æ ‡æ³¨
    * æ”¶é›†çš„æ•°æ®æœ‰ä¸€éƒ¨åˆ†æ˜¯ä¸åŒ…å«çš„ä½ç½®æ ‡ç­¾çš„ï¼Œä»…ä»…æ˜¯(é—®é¢˜-æ–‡ç« -ç­”æ¡ˆ)çš„ä¸‰å…ƒç»„å½¢å¼ã€‚
      æ‰€ä»¥ï¼Œå¯¹äºåªæœ‰ç­”æ¡ˆè€Œæ²¡æœ‰ä½ç½®æ ‡ç­¾çš„æ•°æ®é€šè¿‡æ­£åˆ™åŒ¹é…è¿›è¡Œä½ç½®æ ‡æ³¨ï¼š<br/>
      â‘  è‹¥ç­”æ¡ˆç‰‡æ®µå¤šæ¬¡å‡ºç°åœ¨æ–‡ç« ä¸­ï¼Œé€‰æ‹©ä¸Šä¸‹æ–‡ä¸é—®é¢˜æœ€ç›¸ä¼¼çš„ç­”æ¡ˆç‰‡æ®µä½œä¸ºæ ‡å‡†ç­”æ¡ˆï¼ˆä½¿ç”¨F1å€¼è®¡ç®—ç›¸ä¼¼åº¦ï¼Œç­”æ¡ˆç‰‡æ®µçš„ä¸Šæ–‡48å’Œä¸‹æ–‡48ä¸ªå­—ç¬¦ä½œä¸ºä¸Šä¸‹æ–‡ï¼‰ï¼›<br/>
      â‘¡ è‹¥ç­”æ¡ˆç‰‡æ®µåªå‡ºç°ä¸€æ¬¡ï¼Œåˆ™é»˜è®¤è¯¥ç­”æ¡ˆä¸ºæ ‡å‡†ç­”æ¡ˆã€‚
    * é‡‡ç”¨æ»‘åŠ¨çª—å£å°†é•¿æ–‡æ¡£åˆ‡åˆ†ä¸ºå¤šä¸ªé‡å çš„å­æ–‡æ¡£ï¼Œæ•…ä¸€ä¸ªæ–‡æ¡£å¯èƒ½ä¼šç”Ÿæˆå¤šä¸ªæœ‰ç­”æ¡ˆçš„å­æ–‡æ¡£ã€‚
  * æ— ç­”æ¡ˆæ•°æ®æ„é€ 
    * åœ¨è·¨é¢†åŸŸæ•°æ®ä¸Šè®­ç»ƒå¯ä»¥å¢åŠ æ•°æ®çš„é¢†åŸŸå¤šæ ·æ€§ï¼Œè¿›è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œè´Ÿæ ·æœ¬çš„å¼•å…¥æ°å¥½èƒ½ä½¿å¾—æ¨¡å‹ç¼–ç å°½å¯èƒ½å¤šçš„æ•°æ®ï¼ŒåŠ å¼ºæ¨¡å‹å¯¹éš¾æ ·æœ¬çš„è¯†åˆ«èƒ½åŠ›ï¼š<br/>
      â‘  å¯¹äºæ¯ä¸€ä¸ªé—®é¢˜ï¼Œéšæœºä»æ•°æ®ä¸­æå–contextï¼Œå¹¶ä¿ç•™å¯¹åº”çš„titleä½œä¸ºè´Ÿæ ·æœ¬;ï¼ˆ50%ï¼‰<br/>
      â‘¡ å¯¹äºæ¯ä¸€ä¸ªé—®é¢˜ï¼Œå°†å…¶æ­£æ ·æœ¬ä¸­ç­”æ¡ˆå‡ºç°çš„å¥å­åˆ é™¤ï¼Œä»¥æ­¤ä½œä¸ºè´Ÿæ ·æœ¬ï¼›ï¼ˆ20%ï¼‰<br/>
      â‘¢ å¯¹äºæ¯ä¸€ä¸ªé—®é¢˜ï¼Œä½¿ç”¨BM25ç®—æ³•å¬å›å¾—åˆ†æœ€é«˜çš„å‰åä¸ªæ–‡æ¡£ï¼Œç„¶åæ ¹æ®å¾—åˆ†é‡‡æ ·å‡ºä¸€ä¸ªcontextä½œä¸ºè´Ÿæ ·æœ¬ï¼Œ
      å¯¹äºéå®ä½“ç±»ç­”æ¡ˆï¼Œå‰”é™¤å¾—åˆ†æœ€é«˜çš„contextï¼ˆ30%ï¼‰
* **ç”¨é€”**  
  * æ­¤mrcæ¨¡å‹å¯ç›´æ¥ç”¨äº`open domain`ï¼Œ[ç‚¹å‡»ä½“éªŒ](https://huggingface.co/luhua/chinese_pretrain_mrc_roberta_wwm_ext_large)
  * å°†æ­¤æ¨¡å‹æ”¾åˆ°ä¸‹æ¸¸ MRC/åˆ†ç±» ä»»åŠ¡å¾®è°ƒå¯æ¯”ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æé«˜`2ä¸ªç‚¹`/`1ä¸ªç‚¹`ä»¥ä¸Š
* **åˆä½œ**
  * ç›¸å…³è®­ç»ƒæ•°æ®ä»¥åŠä½¿ç”¨æ›´å¤šæ•°æ®è®­ç»ƒçš„æ¨¡å‹/ä¸€èµ·æ‰“æ¯”èµ› å¯é‚®ç®±è”ç³»(luhua98@foxmail.com)~ 
  
```
----- ä½¿ç”¨æ–¹æ³• -----
from transformers import AutoTokenizer, AutoModelForQuestionAnswering

model_name = "chinese_pretrain_mrc_roberta_wwm_ext_large" # "chinese_pretrain_mrc_macbert_large"

# Use in Transformers
tokenizer = AutoTokenizer.from_pretrained(f"luhua/{model_name}")
model = AutoModelForQuestionAnswering.from_pretrained(f"luhua/{model_name}")

# Use locallyï¼ˆé€šè¿‡ https://huggingface.co/luhua ä¸‹è½½æ¨¡å‹åŠé…ç½®æ–‡ä»¶ï¼‰
tokenizer = BertTokenizer.from_pretrained(f'./{model_name}')
model = AutoModelForQuestionAnswering.from_pretrained(f'./{model_name}')
```

## äºŒã€ä»“åº“ä»‹ç»
* **ç›®çš„**
  * **å¼€æºäº†åŸºäºMRCæ•°æ®å†è®­ç»ƒçš„æ¨¡å‹**ï¼Œåœ¨MRCä»»åŠ¡ä¸‹å¾®è°ƒï¼Œæ•ˆæœå¤§å¹…ä¼˜äºä½¿ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œå…¶æ¬¡ï¼Œæ—¨åœ¨æä¾›ä¸€ä¸ªæ•ˆæœä¸é”™çš„`å¼ºåŸºçº¿`
  * æœ‰äº›[mrcæ¯”èµ›](#æ¯”èµ›)ç”±äº"å¹´ä»£ä¹…è¿œ"æ•´ç†ä¸è¿‡æ¥ï¼ˆ`others`æ–‡ä»¶å¤¹ï¼‰ï¼Œä½†æ–¹æ¡ˆå’Œä»£ç éƒ½æœ‰ï¼Œå¯¹æ¯”ç€çœ‹å°±çœ‹æ‡‚äº†
* **ä¼˜åŒ–**
  * ä»£ç åŸºäºHugginfaceçš„squadä»£ç ã€‚ä¹‹å‰è‡ªå·±å¼€å‘ï¼Œç‰ˆæœ¬å¤šä¸”è®¸å¤šç»†èŠ‚æ²¡æœ‰è€ƒè™‘ï¼Œä¾¿è½¬ç§»åˆ°squadä»£ç ä¸Šè¿­ä»£ã€‚ä½†å…¶å®ç°çš„ç±»ç¼ºä¹å¯¹ä¸­æ–‡çš„æ”¯æŒï¼Œæ¨ç†ç»“æœæœ‰ä¸€äº›å½±å“ï¼Œ**ä¿®æ”¹ä¹‹å æ­¤åº“èƒ½è¾ƒå¥½çš„æ”¯æŒä¸­æ–‡ï¼ŒæŠ½å–çš„ç­”æ¡ˆç²¾åº¦ä¹Ÿå°½å¯èƒ½ä¸å—å½±å“**
  

## ä¸‰ã€æ¯”èµ›

* [ç–«æƒ…æ”¿åŠ¡é—®ç­”åŠ©æ‰‹ ç¬¬ä¸€](https://www.datafountain.cn/competitions/424)
* [Dureader-2021è¯­è¨€ä¸æ™ºèƒ½æŠ€æœ¯ç«èµ› ç¬¬ä¸‰](https://aistudio.baidu.com/aistudio/competition/detail/66?isFromLuge=true)
* [Dureader-2020è¯­è¨€ä¸æ™ºèƒ½æŠ€æœ¯ç«èµ› ç¬¬äºŒ](https://aistudio.baidu.com/aistudio/competition/detail/28?isFromCcf=true)
* [Dureader-2019è¯­è¨€ä¸æ™ºèƒ½æŠ€æœ¯ç«èµ› ç¬¬äº”](https://ai.baidu.com/broad/leaderboard?dataset=dureader)
* [æˆè¯­é˜…è¯»ç†è§£ ç¬¬äºŒ](https://www.biendata.xyz/competition/idiom/)
* [è±æ–¯æ¯å†›äº‹é˜…è¯»ç†è§£ ç¬¬ä¸‰](https://www.heywhale.com/home/competition/5d142d8cbb14e6002c04e14a/leaderboard)


## å››ã€è¿è¡Œæµç¨‹

è„šæœ¬å‚æ•°è§£é‡Š

* `--lm`: è¦åŠ è½½çš„æ¨¡å‹çš„æ–‡ä»¶å¤¹åç§°
* `--do_train`: å¼€å¯è®­ç»ƒ
* `--evaluate_during_training`: å¼€å¯è®­ç»ƒæ—¶çš„éªŒè¯
* `--do_test`:  å¼€å¯é¢„æµ‹
* `--version_2_with_negative`: å¼€å¯é€‚é…äºæ•°æ®ä¸­æœ‰`æ— ç­”æ¡ˆæ•°æ®`ï¼ˆå¦‚ï¼šsquad2.0ã€dureader2021ï¼‰
* `--threads`: æ•°æ®å¤„ç†æ‰€ä½¿ç”¨çš„çº¿ç¨‹æ•°ï¼ˆå¯ä»¥é€šè¿‡os.cpu_count()æŸ¥çœ‹æœºå™¨æ”¯æŒçš„çº¿ç¨‹æ•°ï¼‰
  
##### ä¸€ã€æ•°æ® & æ¨¡å‹ï¼š
* å°†trainã€devã€testç­‰æ•°æ®æ”¾åœ¨datasetsæ–‡ä»¶å¤¹ä¸‹(æ ·ä¾‹æ•°æ®å·²ç»™å‡ºï¼Œç¬¦åˆæ ¼å¼å³å¯)
* é€šè¿‡ export lm=xxx æŒ‡å®šæ¨¡å‹ç›®å½•

##### äºŒã€ä¸€é”®è¿è¡Œ
```python 
sh train_bert.sh  # sh test_bert.sh
```

##### ä¸‰ã€æ— ç­”æ¡ˆé—®é¢˜
* å¦‚æœåŒ…å«æ— ç­”æ¡ˆç±»å‹æ•°æ®ï¼ˆå¦‚ï¼šsquad2.0ã€dureader2021ï¼‰ï¼ŒåŠ å…¥--version_2_with_negativeå°±è¡Œ
* å°†æ•°æ®æ›¿æ¢ä¸ºDureader2021_checklistçš„æ•°æ®, åŠ å…¥--version_2_with_negativeå³å¯


## äº”ã€å°å°æç¤ºï¼š
* ä»£ç ä¸Šä¼ å‰å·²ç»è·‘é€šã€‚æ–‡ä»¶ä¸å¤šï¼Œæ‰€ä»¥å¦‚æœç¢°åˆ°æŠ¥é”™ä¹‹ç±»çš„ä¿¡æ¯ï¼Œå¯èƒ½æ˜¯ä»£ç è·¯å¾„ä¸å¯¹ã€ç¼ºå°‘å®‰è£…åŒ…ç­‰é—®é¢˜ï¼Œä¸€æ­¥æ­¥è§£å†³ï¼Œå¯ä»¥æissue
* ç¯å¢ƒ
  ```
  pip install transformers==2.10.0 
  ```
* ä»£ç åŸºäºtransformers 2.10.0ç‰ˆæœ¬ï¼Œä½†æ˜¯é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥ä½¿ç”¨å…¶ä»–ç‰ˆæœ¬åŠ è½½ã€‚è½¬æ¢ä¸ºtfå¯ä½¿ç”¨[è½¬æ¢](https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/convert_bert_pytorch_checkpoint_to_original_tf.py)
* é¢„è®­ç»ƒç›¸å…³å‚æ•° [å‚è€ƒ](https://github.com/basketballandlearn/MRC_Competition_Dureader/issues/33)

## å…­ã€æ¨¡å‹ä»‹ç»

### 6.1ã€æ¨¡å‹å‚æ•°é…ç½®æ–¹æ³•

```python
import argparse
parser = argparse.ArgumentParser()
parser.add_argument("--server_ip", type=str, default="", help="Can be used for distant debugging.")
parser.add_argument("--null_score_diff_threshold",type=float,default=0.0,help="If null_score - best_non_null is greater than the threshold predict null.",)
parser.add_argument("--do_eval", default=True,action="store_true", help="Whether to run eval on the dev set.")
parser.add_argument("--threads", type=int, default=3, help="multiple threads for converting example to features")
parser.add_argument("--local_rank", type=int, default=-1, help="local_rank for distributed training on gpus")
args = parser.parse_args()
# æ³¨æ„æ¯ä¸ªè¿›ç¨‹åˆ†é…ä¸€ä¸ª local_rank å‚æ•°ï¼Œè¡¨ç¤ºå½“å‰è¿›ç¨‹åœ¨å½“å‰ä¸»æœºä¸Šçš„ç¼–å·ã€‚ä¾‹å¦‚ï¼šrank=2, local_rank=0 è¡¨ç¤ºç¬¬ 3 ä¸ªèŠ‚ç‚¹ä¸Šçš„ç¬¬ 1 ä¸ªè¿›ç¨‹ã€‚
```

### 6.2ã€é¢„è®­ç»ƒæ¨¡å‹åŠ è½½

```python
    from transformers import (
    BertConfig,
    BertTokenizer,
    AutoModelForQuestionAnswering,)
    # åŠ è½½æ¨¡å‹é…ç½®æ–‡ä»¶
    config = BertConfig.from_pretrained(
    args.config_name if args.config_name else args.model_name_or_path,
    cache_dir=args.cache_dir if args.cache_dir else None,
    )
    # åŠ è½½æ¨¡å‹tokenizer
    tokenizer = BertTokenizer.from_pretrained(
    args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,
    do_lower_case=args.do_lower_case,
    cache_dir=args.cache_dir if args.cache_dir else None,
    )
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    model = AutoModelForQuestionAnswering.from_pretrained(
    args.model_name_or_path,
    from_tf=bool(".ckpt" in args.model_name_or_path),
    config=config,
    cache_dir=args.cache_dir if args.cache_dir else None,
    )
    # å°†æ¨¡å‹åŠ è½½åˆ°è®¾å¤‡
    model.to(args.device)
```

```
# æ¨¡å‹åŠ è½½ç»“æœ
03/17/2022 14:38:47 - INFO - transformers.configuration_utils -   loading configuration file G:\MRC\MRC_Competition_Dureader-master\pretrain_model/config.json
03/17/2022 14:38:47 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "architectures": [
    "BertForQuestionAnswering"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 21128
}
03/17/2022 14:38:47 - INFO - transformers.tokenization_utils -   Model name 'G:\MRC\MRC_Competition_Dureader-master\pretrain_model/vocab.txt' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'G:\MRC\MRC_Competition_Dureader-master\pretrain_model/vocab.txt' is a path, a model identifier, or url to a directory containing tokenizer files.
03/17/2022 14:38:47 - WARNING - transformers.tokenization_utils -   Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated
03/17/2022 14:38:47 - INFO - transformers.tokenization_utils -   loading file G:\MRC\MRC_Competition_Dureader-master\pretrain_model/vocab.txt
03/17/2022 14:38:47 - INFO - transformers.modeling_utils -   loading weights file G:\MRC\MRC_Competition_Dureader-master\pretrain_model\pytorch_model.bin
```

### 6.3ã€æ•°æ®å¤„ç†è¿‡ç¨‹

```python
#å°†è¾“å…¥çš„æ–‡æœ¬å¤„ç†æˆ
def load_and_cache_examples(args, tokenizer, set_type='train', output_examples=False):
    global examples
    # Load data features from cache or dataset file
    input_dir = args.feature_dir if args.feature_dir else "."
    cached_features_file = os.path.join(
        input_dir,
        "cached_{}_{}".format(
            set_type,
            str(args.max_seq_length),
        ),
    )
    # Init features and dataset from cache if it exists
    if os.path.exists(cached_features_file) and not args.overwrite_cache:
        logger.info("Loading features from cached file %s", cached_features_file)
        features_and_dataset = torch.load(cached_features_file)
        features, dataset, examples = (
            features_and_dataset["features"],
            features_and_dataset["dataset"],
            features_and_dataset["examples"],)
    else:
        logger.info("Creating features from dataset file at %s", input_dir)

        processor = MyProcessor()
        if set_type == 'dev':
            examples = processor.get_dev_examples(args.data_dir, filename=args.predict_file)
        elif set_type == 'train':
            examples = processor.get_train_examples(args.data_dir, filename=args.train_file)
        elif set_type == 'test':
            examples = processor.get_test_examples(args.data_dir, filename=args.test_file)

        features, dataset = squad_convert_examples_to_features_orig(
            examples=examples,
            tokenizer=tokenizer,
            max_seq_length=args.max_seq_length,
            doc_stride=args.doc_stride,
            max_query_length=args.max_query_length,
            is_training=set_type == 'train',
            return_dataset="pt",
            threads=args.threads,
        )
		# å°†å¤„ç†å¥½çš„æ•°æ®è¿›è¡Œå†™å…¥æœ¬åœ°ä¿å­˜
        if args.local_rank in [-1, 0]:
            logger.info("Saving features into cached file %s", cached_features_file)
            torch.save({"features": features, "dataset": dataset, "examples": examples}, cached_features_file)
    is_evaluate= set_type == 'train'
    if args.local_rank == 0 and is_evaluate:
        # Make sure only the first process in distributed training process the dataset, and the others will use the cache
        torch.distributed.barrier()

    if output_examples:
        return dataset, examples, features
    return dataset
```

åœ¨æ„å»ºè®­ç»ƒæ•°æ®çš„æ—¶å€™ï¼Œset_type ä¸º 'train'æ—¶ï¼Œprocessor.get_train_examples(ï¼‰å‡½æ•°ä¼šé€šè¿‡è¯»å–æœ¬åœ°è®­ç»ƒæ•°æ®ï¼Œä¼šå°†æ•°æ®å¤„ç†æˆä¸‹é¢çš„å½¢å¼ã€‚æ•°æ®åŒ…æ‹¬é—®é¢˜ã€ç­”æ¡ˆå’Œå¯¹åº”çš„æ–‡æœ¬ç­‰çš„å½¢å¼ã€‚set_type ä¸ºtestå’Œdevå½¢å¼è¿‡ç¨‹éƒ½æ˜¯ä¸€æ ·çš„ã€‚

![image-20220317144521038](README/image-20220317144521038.png)

ç„¶åå°†å¾—åˆ°çš„examplesé€šè¿‡squad_convert_examples_to_features_orig()å°†ç¤ºä¾‹åˆ—è¡¨è½¬æ¢ä¸ºå¯ç›´æ¥ä½œä¸ºæ¨¡å‹è¾“å…¥çš„ç‰¹æ€§åˆ—è¡¨ã€‚  å®ƒä¾èµ–äºæ¨¡å‹ï¼Œå¹¶åˆ©ç”¨è®¸å¤šæ ‡è®°å™¨çš„ç‰¹æ€§æ¥åˆ›å»ºæ¨¡å‹çš„è¾“å…¥ã€‚  ä½œè€…åº”è¯¥æ˜¯ä»¿ç…§datasetsæ•°æ®çš„æ ·å¼ï¼Œå°†æ¨¡å‹è®­ç»ƒæ•°æ®å¤„ç†æˆdatasetsæ ‡æ³¨å½¢å¼ã€‚

### 6.4ã€æ¨¡å‹è®­ç»ƒè¿‡ç¨‹

```python
def train(args, train_dataset, model, tokenizer):
    """ Train the model """
    if args.local_rank in [-1, 0]:
        tb_writer = SummaryWriter(args.summary)
	# è®¡ç®—gpuè®­ç»ƒæ‰€éœ€batch_size
    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)
    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)
    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)
	# è®¡ç®—æ¨¡å‹è®­ç»ƒæ‰€éœ€è¦å¤šå°‘æ—¶é—´æ­¥
    if args.max_steps > 0:
        t_total = args.max_steps
        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1
    else:
        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs

    warmup_steps = int(t_total * args.warmup_ratio)
    logging_steps = int(t_total * args.logging_ratio)
    save_steps = int(t_total * args.save_ratio)

```

ï¼ˆ1ï¼‰è®¡ç®—æ¨¡å‹è®­ç»ƒä¸€å…±éœ€è¦å¤šå°‘æ—¶é—´æ­¥ï¼Œç„¶ååˆ†åˆ«è®¡ç®—warmup_stepsã€logging_stepså’Œsave_stepsã€‚

![image-20220317152119210](README/image-20220317152119210.png)

    # Prepare optimizer and schedule (linear warmup and decay)
    no_decay = ["bias", "LayerNorm.weight"]
    optimizer_grouped_parameters = [
        {
            "params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
            "weight_decay": args.weight_decay,
        },
        {"params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], "weight_decay": 0.0},
    ]
    if args.gc:
        optimizer = AdamW_GC(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)
    else:
        optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)
    # optimizer = Lookahead(optimizer=optimizer, k=5, alpha=0.5)
    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total
    )
    
    # Check if saved optimizer or scheduler states exist
    if os.path.isfile(os.path.join(args.model_name_or_path, "optimizer.pt")) and os.path.isfile(
        os.path.join(args.model_name_or_path, "scheduler.pt")
    ):
        # Load in optimizer and scheduler states
        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, "optimizer.pt")))
        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, "scheduler.pt")))
    
    # multi-gpu training (should be after apex fp16 initialization)
    if args.n_gpu > 1:
        model = torch.nn.DataParallel(model)
    # Train!
    logger.info("***** Running training *****")
    logger.info("  Num examples = %d", len(train_dataset))
    logger.info("  Num Epochs = %d", args.num_train_epochs)
    logger.info("  Instantaneous batch size per GPU = %d", args.per_gpu_train_batch_size)
    logger.info(
        "  Total train batch size (w. parallel, distributed & accumulation) = %d",
        args.train_batch_size
        * args.gradient_accumulation_steps
        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),
    )
    logger.info("  Gradient Accumulation steps = %d", args.gradient_accumulation_steps)
    logger.info("  Total optimization steps = %d", t_total)
    
    global_step = 1
    epochs_trained = 0
    steps_trained_in_current_epoch = 0
    # Check if continuing training from a checkpoint
    if os.path.exists(args.model_name_or_path):
        try:
            # set global_step to gobal_step of last saved checkpoint from model path
            checkpoint_suffix = args.model_name_or_path.split("-")[-1].split("/")[0]
            global_step = int(checkpoint_suffix)
            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)
            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)
    
            logger.info("  Continuing training from checkpoint, will skip to saved global_step")
            logger.info("  Continuing training from epoch %d", epochs_trained)
            logger.info("  Continuing training from global step %d", global_step)
            logger.info("  Will skip the first %d steps in the first epoch", steps_trained_in_current_epoch)
            steps_trained_in_current_epoch *= args.gradient_accumulation_steps
        except ValueError:
            logger.info("  Starting fine-tuning.")
    
    tr_loss, logging_loss = 0.0, 0.0
    model.zero_grad()
    train_iterator = trange(
        epochs_trained, int(args.num_train_epochs), desc="Epoch", disable=args.local_rank not in [-1, 0]
    )
    # Added here for reproductibility
    set_seed(args)
    
    for _ in train_iterator:
        epoch_iterator = tqdm(train_dataloader, desc="Iteration", disable=args.local_rank not in [-1, 0])
        for step, batch in enumerate(epoch_iterator):
    
            # Skip past any already trained steps if resuming training
            if steps_trained_in_current_epoch > 0:
                steps_trained_in_current_epoch -= 1
                continue
    
            model.train()
            batch = tuple(t.to(args.device) for t in batch)
    
            inputs = {
                "input_ids": batch[0],
                "attention_mask": batch[1],
                "token_type_ids": batch[2],
                "start_positions": batch[3],
                "end_positions": batch[4],
            }

ï¼ˆ2ï¼‰åŠ è½½å†™å…¥æœ¬åœ°çš„ç¼“å­˜è®­ç»ƒæ•°æ®è¿›è¡Œï¼Œç”Ÿæˆæ¨¡å‹è®­ç»ƒæ•°æ®ï¼Œå…¶ä¸­æ¨¡å‹è®­ç»ƒæ•°æ®åŒ…æ‹¬input_ids,attention_mask,token_type_idsï¼Œstart_positionså’Œend_positionsã€‚

![image-20220317152519030](README/image-20220317152519030.png)

input_idså¯¹åº”çš„tensoræ•°æ®

![image-20220317152747599](README/image-20220317152747599.png)

attention_maskå¯¹åº”çš„tensoræ•°æ®

![image-20220317152848162](README/image-20220317152848162.png)

        outputs = model(**inputs)
        # model outputs are always tuple in transformers (see doc)
        loss = outputs[0]

ï¼ˆ3ï¼‰æŸå¤±è®¡ç®—ï¼Œtrain_lossæŸå¤±ä½¿ç”¨åŸç”Ÿçš„bertè¿›è¡Œæ¨¡å‹è®­ç»ƒçš„ã€‚

![image-20220317153511865](README/image-20220317153511865.png)

```python
#è®¡ç®—æŸå¤±çš„æ—¶å€™ï¼Œéœ€è¦è°ƒç”¨trasformerså®šä¹‰å¥½é—®ç­”bert_modelï¼Œç„¶åè¿›è¡ŒæŸå¤±è®¡ç®—ã€‚
# model = AutoModelForQuestionAnswering.from_pretrained()
class BertForQuestionAnswering(BertPreTrainedModel):
    def __init__(self, config):
        super(BertForQuestionAnswering, self).__init__(config)
        self.num_labels = config.num_labels

        self.bert = BertModel(config)
        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)

        self.init_weights()

    @add_start_docstrings_to_callable(BERT_INPUTS_DOCSTRING)
    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        start_positions=None,
        end_positions=None,
    ):

        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
        )

        sequence_output = outputs[0] #outputs: last_hidden_state, pooler_output, (hidden_states), (attentions)

        logits = self.qa_outputs(sequence_output)
        start_logits, end_logits = logits.split(1, dim=-1) # åˆ†ç¦»å‡ºçš„start_logits/end_logitså½¢çŠ¶ä¸º([config.hidden_size, 1])
        start_logits = start_logits.squeeze(-1) # é™ç»´è‡³å½¢çŠ¶ä¸º([config.hidden_size])
        end_logits = end_logits.squeeze(-1) # é™ç»´è‡³å½¢çŠ¶ä¸º([config.hidden_size])

        outputs = (start_logits, end_logits,) + outputs[2:]
        if start_positions is not None and end_positions is not None:
            # If we are on multi-GPU, split add a dimension
            if len(start_positions.size()) > 1:
                start_positions = start_positions.squeeze(-1)
            if len(end_positions.size()) > 1:
                end_positions = end_positions.squeeze(-1)
            # sometimes the start/end positions are outside our model inputs, we ignore these terms
            ignored_index = start_logits.size(1)
            start_positions.clamp_(0, ignored_index)
            end_positions.clamp_(0, ignored_index)

            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)
            start_loss = loss_fct(start_logits, start_positions)
            end_loss = loss_fct(end_logits, end_positions)
            total_loss = (start_loss + end_loss) / 2
            outputs = (total_loss,) + outputs

        return outputs  # (loss), start_logits, end_logits, (hidden_states), (attentions)
```

### 6.5ã€æ¨¡å‹è§£ç è¿‡ç¨‹

æ¨¡å‹éªŒè¯éƒ¨åˆ†ä»£ç ï¼Œè¯¥éƒ¨åˆ†åŒ…æ‹¬è§£

```python
def evaluate(args, model, tokenizer, prefix="dev", step=0):
    # å°†æ¨¡å‹ç”¨äºéªŒè¯å’Œé¢„æµ‹çš„æ•°æ®è¿›è¡Œå¤„ç†
    dataset, examples, features = load_and_cache_examples(args, tokenizer, set_type=prefix, output_examples=True)
    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:
        os.makedirs(args.output_dir)
	# è®¡ç®—æ¨¡å‹æ‰€éœ€è¦çš„batch_size
    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)
    # æ³¨æ„DistributedSampleréšæœºæŠ½æ ·
    eval_sampler = SequentialSampler(dataset)
    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)
    # multi-gpu evaluate
    if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):
        model = torch.nn.DataParallel(model)
    all_results = []
    start_time = timeit.default_timer()
    for batch in tqdm(eval_dataloader, desc="Evaluating"):
        model.eval()
        batch = tuple(t.to(args.device) for t in batch)
        with torch.no_grad():
            inputs = {
                "input_ids": batch[0],
                "attention_mask": batch[1],
                "token_type_ids": batch[2],
            }
            example_indices = batch[3]
            outputs = model(**inputs)
        for i, example_index in enumerate(example_indices):
            eval_feature = features[example_index.item()]
            unique_id = int(eval_feature.unique_id)
            output = [to_list(output[i]) for output in outputs]
            # Some models (XLNet, XLM) use 5 arguments for their predictions, while the other "simpler"
            # è·å–æ¨¡å‹è¾“å‡ºï¼Œoutputåˆ†åˆ«å¯¹åº”ä¸åŒæ•°æ®
            if len(output) >= 5:
                start_logits = output[0]
                start_top_index = output[1]
                end_logits = output[2]
                end_top_index = output[3]
                cls_logits = output[4]
                result = SquadResult(
                    unique_id,
                    start_logits,
                    end_logits,
                    start_top_index=start_top_index,
                    end_top_index=end_top_index,
                    cls_logits=cls_logits,
                )

            else:
                start_logits, end_logits = output
                result = SquadResult(unique_id, start_logits, end_logits)
            all_results.append(result)
```

(1)æ¨¡å‹é¢„æµ‹æ—¶ï¼Œå¦‚æœoutputçš„é•¿åº¦è¶…è¿‡5ï¼Œåˆ™è®¤ä¸ºoutput[0]ä¸ºstart_logits,output[1]ä¸ºstart_top_indexã€‚ã€‚ã€‚

![image-20220317170216067](README/image-20220317170216067.png)

åä¹‹ï¼Œoutputé•¿åº¦å°äº5åˆ™ä¸ºï¼Œstart_logitsï¼Œend_logitsåˆ†åˆ«ä¸ºout[1],out[2]ã€‚æ¨¡å‹é¢„æµ‹æ¯ä¸ªå­—ç¬¦çš„å¼€å§‹å’Œç»“æŸlogitsã€‚

![image-20220317170640413](README/image-20220317170640413.png)

![image-20220317170820832](README/image-20220317170820832.png)

ç„¶åè·å¾—start_logitsï¼Œend_logitsç»è¿‡SquadResult()å‡½æ•°ï¼Œæ²¡æœ‰ç»è¿‡ä»€ä¹ˆå¤„ç†ï¼Œç„¶åè¿”å›äº†resultï¼Œå…¶ä¸­unique_idä¸ºé—®é¢˜çš„idã€‚resultå…¶å®å°±æ˜¯ä¸€ä¸ªé—®é¢˜å¯¹åº”çš„ç­”æ¡ˆç»“æœå¼€å§‹å’Œç»“æŸçš„logitsã€‚

![image-20220317171025219](README/image-20220317171025219.png)

![image-20220317171808288](README/image-20220317171808288.png)

```
    evalTime = timeit.default_timer() - start_time
    logger.info("  Evaluation done in total %f secs (%f sec per example)", evalTime, evalTime / len(dataset))
    try:
        # Compute predictions
        output_prediction_file = os.path.join(args.output_dir, "predictions_{}_{}.json".format(prefix, step))
        output_nbest_file = os.path.join(args.output_dir, "nbest_predictions_{}_{}.json".format(prefix, step))
        assert isinstance(int(step),int)
    except:
        print(step)
	# æ— ç­”æ¡ˆè§£æè®¾ç½®
    if args.version_2_with_negative:
        output_null_log_odds_file = os.path.join(args.output_dir, "null_odds_{}_{}.json".format(prefix, step))
    else:
        output_null_log_odds_file = None

    # æ¨¡å‹è®¡ç®—é¢„æµ‹logitså‡½æ•°ï¼Œå†…å«ç­”æ¡ˆè§£ç è¿‡ç¨‹
    predictions = compute_predictions_logits(
        examples,
        features,
        all_results,
        args.n_best_size,
        args.max_answer_length,
        args.do_lower_case,
        output_prediction_file,
        output_nbest_file,
        output_null_log_odds_file,
        args.verbose_logging,
        args.version_2_with_negative,
        args.null_score_diff_threshold,
        tokenizer
    )

    if prefix == 'dev':
        # Compute the F1 and exact scores.
        results = squad_evaluate(examples, predictions)
        return results
    else:
        return None
```

ï¼ˆ2ï¼‰è§£ç éƒ¨åˆ†

```python
def compute_predictions_logits(
    all_examples,
    all_features,
    all_results,
    n_best_size,
    max_answer_length,
    do_lower_case,
    output_prediction_file,
    output_nbest_file,
    output_null_log_odds_file,
    verbose_logging,
    version_2_with_negative,
    null_score_diff_threshold,
    tokenizer,
):
    """Write final predictions to the json file and log-odds of null if needed."""
    logger.info("Writing predictions to: %s" % (output_prediction_file))
    logger.info("Writing nbest to: %s" % (output_nbest_file))
    example_index_to_features = collections.defaultdict(list)
    for feature in all_features:
        example_index_to_features[feature.example_index].append(feature)

    unique_id_to_result = {}
    for result in all_results:
        unique_id_to_result[result.unique_id] = result

    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name
        "PrelimPrediction", ["feature_index", "start_index", "end_index", "start_logit", "end_logit"]
    )

    all_predictions = collections.OrderedDict()
    all_nbest_json = collections.OrderedDict()
    scores_diff_json = collections.OrderedDict()

    for (example_index, example) in enumerate(all_examples):
        features = example_index_to_features[example_index]
```

è·å–æ‰€æœ‰éœ€è¦è¿›è¡Œè§£ç çš„æ•°æ®ï¼Œè¯¥ç¤ºä¾‹é—®é¢˜çš„idä¸º1000000000ï¼Œå®ƒçš„å…¶ä»–ä¿¡æ¯å¦‚input_idï¼Œè¯¦è§ä¸‹å›¾ã€‚

![image-20220317172410323](README/image-20220317172410323.png)

å°†ä¸Šä¸€æ­¥è·å–åˆ°çš„æ•°æ®è¿›è¡Œidæ˜ å°„

```
    unique_id_to_result = {}
    for result in all_results:
        unique_id_to_result[result.unique_id] = result
```

![image-20220317172930931](README/image-20220317172930931.png)

![image-20220317173007862](README/image-20220317173007862.png)

```
all_predictions = collections.OrderedDict()
    all_nbest_json = collections.OrderedDict()
    scores_diff_json = collections.OrderedDict()
    for (example_index, example) in enumerate(all_examples):
        features = example_index_to_features[example_index]
```

![image-20220317173226396](README/image-20220317173226396.png)

```
prelim_predictions = []
        # keep track of the minimum score of null start+end of position 0
        score_null = 1000000  # large and positive
        min_null_feature_index = 0  # the paragraph slice with min null score
        null_start_logit = 0  # the start logit at the slice with min null score
        null_end_logit = 0  # the end logit at the slice with min null score
        for (feature_index, feature) in enumerate(features):
            result = unique_id_to_result[feature.unique_id]
            start_indexes = _get_best_indexes(result.start_logits, n_best_size)
            end_indexes = _get_best_indexes(result.end_logits, n_best_size)
```

![image-20220317173908731](README/image-20220317173908731.png)

æ¨¡å‹è®¾ç½®çš„è¶…å‚æ•°ï¼Œå°±æ˜¯éœ€è¦ç”Ÿæˆå¯¹åº”çš„å¤šå°‘çš„ç­”æ¡ˆã€‚

![image-20220317173555193](README/image-20220317173555193.png)

```python
 # æ— ç­”æ¡ˆè§£ç æœ€æ ¸å¿ƒçš„ä¸€æ­¥æ“ä½œï¼Œé¦–é€‰éœ€è¦å°†é¢„æµ‹ç­”æ¡ˆçš„start_logits[0]ä¸end_logits[0]ç›¸åŠ è·å¾—å¾—åˆ†ï¼Œåé¢è®¡ç®—â€œno answerâ€æ—¶ä¼šç”¨åˆ°ã€‚score_nullè¿™é‡Œåº”è¯¥å°±æ˜¯ã€clsã€‘å¯¹åº”çš„start_logitså’Œend_logits
          if version_2_with_negative:
                feature_null_score = result.start_logits[0] + result.end_logits[0]
                if feature_null_score < score_null:
                    score_null = feature_null_score
                    min_null_feature_index = feature_index
                    null_start_logit = result.start_logits[0]
                    null_end_logit = result.end_logits[0]
```

![image-20220317213728392](README/image-20220317213728392.png)

ç„¶åéœ€è¦å¯¹è¿™10ä¸ªé—®é¢˜è¿›è¡Œéå†åˆ¤æ–­ï¼ŒæŠŠæ‰€æœ‰çš„èµ·å§‹ç‚¹å’Œç»“æŸç‚¹å…¨éƒ¨éå†ä¸€éã€‚

```
           for start_index in start_indexes:
                for end_index in end_indexes:
                    # We could hypothetically create invalid predictions, e.g., predict
                    # that the start of the span is in the question. We throw out all
                    # invalid predictions.
                    if start_index >= len(feature.tokens):
                        continue
                    if end_index >= len(feature.tokens):
                        continue
                    if start_index not in feature.token_to_orig_map:
                        continue
                    if end_index not in feature.token_to_orig_map:
                        continue
                    if not feature.token_is_max_context.get(start_index, False):
                        continue
                    if end_index < start_index:
                        continue
                    length = end_index - start_index + 1
                    if length > max_answer_length:
                        continue
```

![image-20220317181623199](README/image-20220317181623199.png)

```
        if version_2_with_negative:
            prelim_predictions.append(
                _PrelimPrediction(
                    feature_index=min_null_feature_index,
                    start_index=0,
                    end_index=0,
                    start_logit=null_start_logit,
                    end_logit=null_end_logit,
                )
            )
        prelim_predictions = sorted(prelim_predictions, key=lambda x: (x.start_logit + x.end_logit), reverse=True)
```

![image-20220317181938659](README/image-20220317181938659.png)

å¯¹é—®é¢˜çš„start_logitså’Œend_logitsç›¸åŠ åçš„ç»“æœè¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œç”Ÿæˆå¯¹åº”çš„æ¦‚ç‡ã€‚

![image-20220317182803432](README/image-20220317182803432.png)

![image-20220317182949662](README/image-20220317182949662.png)

```
        # è®¡ç®—æ— ç­”æ¡ˆè¿‡ç¨‹ï¼Œä½†æ˜¯è¿™å‡ ç§	æƒ…å†µå¾ˆå°‘åœ¨æ¨¡å‹é¢„æµ‹å’ŒéªŒè¯çš„æ—¶å€™å‡ºç°ã€‚
        if version_2_with_negative:
            if "" not in seen_predictions:
                nbest.append(_NbestPrediction(text="no answer", start_logit=null_start_logit, end_logit=null_end_logit))

            # In very rare edge cases we could only have single null prediction.
            # So we just create a nonce prediction in this case to avoid failure.
            if len(nbest) == 1:
                nbest.insert(0, _NbestPrediction(text="no answer", start_logit=0.0, end_logit=0.0))

        # In very rare edge cases we could have no valid predictions. So we
        # just create a nonce prediction in this case to avoid failure.
        if not nbest:
            nbest.append(_NbestPrediction(text="no answer", start_logit=0.0, end_logit=0.0))
```

```python
        # è®¡ç®—å«æœ‰ç­”æ¡ˆçš„ç»“æœçš„è¿‡ç¨‹
        if not version_2_with_negative:
            all_predictions[example.qas_id] = nbest_json[0]["text"]
        else:
			# è®¡ç®—æ— ç­”æ¡ˆçš„è§£ç è¿‡ç¨‹ï¼Œéœ€è¦è®¡ç®—score_nullï¼ˆã€clsã€‘å¯¹åº”çš„start_logitå’Œend_logitä¹‹å’Œï¼‰å‡å»å…¶ä»–é¢„æµ‹å­—ç¬¦çš„start_logitå’Œend_logit,å¦‚æœç»“æœå¤§äºé›¶è¯´æ˜æ— ç­”æ¡ˆã€‚
            score_diff = score_null - best_non_null_entry.start_logit - (best_non_null_entry.end_logit)
            scores_diff_json[example.qas_id] = score_diff
            # null_score_diff_thresholdè¶…å‚è®¾ç½®çš„é˜ˆå€¼ï¼Œåœ¨æ­¤æ¬¡åº”è¯¥ä¸º0.0
            if score_diff > null_score_diff_threshold:
                all_predictions[example.qas_id] = "no answer"
            else:
                all_predictions[example.qas_id] = best_non_null_entry.text
        all_nbest_json[example.qas_id] = nbest_json
```

## ä¸ƒã€æ„Ÿè°¢

[zhangxiaoyu](https://github.com/Decalogue)  [huanghui](https://github.com/huanghuidmml)  [nanfulai](https://github.com/nanfulai)



